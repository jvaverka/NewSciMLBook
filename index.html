<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/NewSciMLBook/css/franklin.css"> <link rel=stylesheet  href="/NewSciMLBook/css/tufte.css"> <link rel=stylesheet  href="/NewSciMLBook/css/latex.css"> <link rel=stylesheet  href="/NewSciMLBook/css/adjust.css"> <link rel=icon  href="/NewSciMLBook/assets/favicon.png"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <title>SciML Book</title> <div id=layout > <div id=menu > <ul> <li><a style="font-size:larger;" href="/NewSciMLBook/">Home</a> <li><a style="font-size:larger;" href="/NewSciMLBook/info/">Info</a> <li><a style="font-size:larger;" href="/NewSciMLBook/homework/">Homework</a> <li><a style="font-size:larger;" href="/NewSciMLBook/lectures/">Lectures</a> <li><a style="font-size:larger;" href="/NewSciMLBook/notes/">Notes</a> <ul style="font-size:smaller"> <li><a href="/NewSciMLBook/notes/02/">02</a> <li><a href="/NewSciMLBook/notes/03/">03</a> <li><a href="/NewSciMLBook/notes/04/">04</a> <li><a href="/NewSciMLBook/notes/05/">05</a> <li><a href="/NewSciMLBook/notes/06/">06</a> <li><a href="/NewSciMLBook/notes/07/">07</a> <li><a href="/NewSciMLBook/notes/08/">08</a> <li><a href="/NewSciMLBook/notes/09/">09</a> <li><a href="/NewSciMLBook/notes/10/">10</a> <li><a href="/NewSciMLBook/notes/11/">11</a> <li><a href="/NewSciMLBook/notes/13/">13</a> <li><a href="/NewSciMLBook/notes/14/">14</a> <li><a href="/NewSciMLBook/notes/15/">15</a> <li><a href="/NewSciMLBook/notes/16/">16</a> <li><a href="/NewSciMLBook/notes/17/">17</a> <li><a href="/NewSciMLBook/notes/18/">18</a> <li><a href="/NewSciMLBook/notes/19/">19</a> </ul> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=sciml_book ><a href="#sciml_book" class=header-anchor >SciML Book</a></h1> <h2 id=parallel_computing_and_scientific_machine_learning_sciml_methods_and_applications ><a href="#parallel_computing_and_scientific_machine_learning_sciml_methods_and_applications" class=header-anchor >Parallel Computing and Scientific Machine Learning &#40;SciML&#41;: Methods and Applications</a></h2> <p><strong>This book is a compilation of lecture notes from the MIT Course 18.337J/6.338J: Parallel Computing and Scientific Machine Learning. Links to the old notes https://mitmath.github.io/18337 will redirect here</strong></p> <p>This repository is meant to be a live document, updating to continuously add the latest details on methods from the field of scientific machine learning and the latest techniques for high-performance computing.</p> <h2 id=introduction_to_parallel_computing_and_scientific_machine_learning ><a href="#introduction_to_parallel_computing_and_scientific_machine_learning" class=header-anchor >Introduction to Parallel Computing and Scientific Machine Learning</a></h2> <p>There are two main branches of technical computing: machine learning and scientific computing. Machine learning has received a lot of hype over the last decade, with techniques such as convolutional neural networks and TSne nonlinear dimensional reductions powering a new generation of data-driven analytics. On the other hand, many scientific disciplines carry on with large-scale modeling through differential equation modeling, looking at stochastic differential equations and partial differential equations describing scientific laws.</p> <p>However, there has been a recent convergence of the two disciplines. This field, scientific machine learning, has been showcasing results like how partial differential equation simulations can be accelerated with neural networks. New methods, such as probabilistic and differentiable programming, have started to be developed specifically for enhancing the tools of this domain. However, the techniques in this field combine two huge areas of computational and numerical practice, meaning that the methods are sufficiently complex. How do you backpropagate an ODE defined by neural networks? How do you perform unsupervised learning of a scientific simulator?</p> <p>In this class we will dig into the methods and understand what they do, why they were made, and thus how to integrate numerical methods across fields to accentuate their pros while mitigating their cons. This class will be a survey of the numerical techniques, showcasing how many disciplines are doing the same thing under different names, and using a common mathematical language to derive efficient routines which capture both data-driven and mechanistic-based modeling.</p> <p>However, these methods will quickly run into a scaling issue if naively coded. To handle this problem, everything will have a focus on performance-engineering. We will start by focusing on algorithm which are inherently serial and learn to optimize serial code. Then we will showcase how logic-heavy code can be parallelized through multithreading and distributed computing techniques like MPI, while direct mathematical descriptions can be parallelized through GPU computing.</p> <p>The final part of the course will be a unique project which pulls together these techniques. As a new field, the students will be exposed to the &quot;low hanging fruit&quot; and will be directed towards an area which they can make a quick impact. For the final project, students will team up to solve a new problem in the field of scientific machine learning, and receive helping writing up a publication-quality analysis about their work.</p> <div class=back-to-top > <span><a href="#" title="Back to Top"><i class="fa fa-chevron-up"></i></a></span> </div> <div class=page-foot > <div class=copyright > &copy; Chris Rackauckas. Last modified: February 21, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>